{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dJIscUr5Ayv"
   },
   "source": [
    "## Homework: Multilingual Embedding-based Machine Translation (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yxqs0-z75Ay3"
   },
   "source": [
    "**In this homework** **<font color='red'>YOU</font>** will make machine translation system without using parallel corpora, alignment, attention, 100500 depth super-cool recurrent neural network and all that kind superstuff.\n",
    "\n",
    "But even without parallel corpora this system can be good enough (hopefully). \n",
    "\n",
    "For our system we choose two kindred Slavic languages: Ukrainian and Russian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMHMIxT35Ay4"
   },
   "source": [
    "### Feel the difference!\n",
    "\n",
    "(_синій кіт_ vs. _синій кит_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4LA9Hrl5Ay4"
   },
   "source": [
    "![blue_cat_blue_whale.png](https://github.com/yandexdataschool/nlp_course/raw/master/resources/blue_cat_blue_whale.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYIN7ll_5Ay4"
   },
   "source": [
    "### Frament of the Swadesh list for some slavic languages\n",
    "\n",
    "The Swadesh list is a lexicostatistical stuff. It's named after American linguist Morris Swadesh and contains basic lexis. This list are used to define subgroupings of languages, its relatedness.\n",
    "\n",
    "So we can see some kind of word invariance for different Slavic languages.\n",
    "\n",
    "\n",
    "| Russian         | Belorussian              | Ukrainian               | Polish             | Czech                         | Bulgarian            |\n",
    "|-----------------|--------------------------|-------------------------|--------------------|-------------------------------|-----------------------|\n",
    "| женщина         | жанчына, кабета, баба    | жінка                   | kobieta            | žena                          | жена                  |\n",
    "| мужчина         | мужчына                  | чоловік, мужчина        | mężczyzna          | muž                           | мъж                   |\n",
    "| человек         | чалавек                  | людина, чоловік         | człowiek           | člověk                        | човек                 |\n",
    "| ребёнок, дитя   | дзіця, дзіцёнак, немаўля | дитина, дитя            | dziecko            | dítě                          | дете                  |\n",
    "| жена            | жонка                    | дружина, жінка          | żona               | žena, manželka, choť          | съпруга, жена         |\n",
    "| муж             | муж, гаспадар            | чоловiк, муж            | mąż                | muž, manžel, choť             | съпруг, мъж           |\n",
    "| мать, мама      | маці, матка              | мати, матір, неня, мама | matka              | matka, máma, 'стар.' mateř    | майка                 |\n",
    "| отец, тятя      | бацька, тата             | батько, тато, татусь    | ojciec             | otec                          | баща, татко           |\n",
    "| много           | шмат, багата             | багато                  | wiele              | mnoho, hodně                  | много                 |\n",
    "| несколько       | некалькі, колькі         | декілька, кілька        | kilka              | několik, pár, trocha          | няколко               |\n",
    "| другой, иной    | іншы                     | інший                   | inny               | druhý, jiný                   | друг                  |\n",
    "| зверь, животное | жывёла, звер, істота     | тварина, звір           | zwierzę            | zvíře                         | животно               |\n",
    "| рыба            | рыба                     | риба                    | ryba               | ryba                          | риба                  |\n",
    "| птица           | птушка                   | птах, птиця             | ptak               | pták                          | птица                 |\n",
    "| собака, пёс     | сабака                   | собака, пес             | pies               | pes                           | куче, пес             |\n",
    "| вошь            | вош                      | воша                    | wesz               | veš                           | въшка                 |\n",
    "| змея, гад       | змяя                     | змія, гад               | wąż                | had                           | змия                  |\n",
    "| червь, червяк   | чарвяк                   | хробак, черв'як         | robak              | červ                          | червей                |\n",
    "| дерево          | дрэва                    | дерево                  | drzewo             | strom, dřevo                  | дърво                 |\n",
    "| лес             | лес                      | ліс                     | las                | les                           | гора, лес             |\n",
    "| палка           | кій, палка               | палиця                  | patyk, pręt, pałka | hůl, klacek, prut, kůl, pálka | палка, пръчка, бастун |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J9tb3ab5Ay5"
   },
   "source": [
    "But the context distribution of these languages demonstrates even more invariance. And we can use this fact for our for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9dFDcUC5Ay5"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U5-2OnTl5Ay5"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wC9en2Vc5Ay6"
   },
   "source": [
    "Download embeddings here:\n",
    "* [cc.uk.300.vec.zip](https://yadi.sk/d/9CAeNsJiInoyUA)\n",
    "* [cc.ru.300.vec.zip](https://yadi.sk/d/3yG0-M4M8fypeQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrAZmL8s5Ay6"
   },
   "source": [
    "Load embeddings for ukrainian and russian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "97uQCgXU5Ay7"
   },
   "outputs": [],
   "source": [
    "uk_emb = KeyedVectors.load_word2vec_format(\"cc.uk.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSPvrLM25Ay7"
   },
   "outputs": [],
   "source": [
    "ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luK_dFLj5Ay7"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([ru_emb[\"август\"]], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QD8SADnJ5Ay8"
   },
   "outputs": [],
   "source": [
    "uk_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVN7zp8T5Ay8"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PXlm_dk5Ay8"
   },
   "source": [
    "Load small dictionaries for correspoinding words pairs as trainset and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlWJ4gUL5Ay8"
   },
   "outputs": [],
   "source": [
    "def load_word_pairs(filename):\n",
    "    uk_ru_pairs = []\n",
    "    uk_vectors = []\n",
    "    ru_vectors = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as inpf:\n",
    "        for line in inpf:\n",
    "            uk, ru = line.rstrip().split(\"\\t\")\n",
    "            if uk not in uk_emb or ru not in ru_emb:\n",
    "                continue\n",
    "            uk_ru_pairs.append((uk, ru))\n",
    "            uk_vectors.append(uk_emb[uk])\n",
    "            ru_vectors.append(ru_emb[ru])\n",
    "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyKT3R3Z5Ay9"
   },
   "outputs": [],
   "source": [
    "uk_ru_train, X_train, Y_train = load_word_pairs(\"ukr_rus.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfgADg7F5Ay9"
   },
   "outputs": [],
   "source": [
    "uk_ru_test, X_test, Y_test = load_word_pairs(\"ukr_rus.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkUFQNqG5Ay-"
   },
   "source": [
    "## Embedding space mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7RxaXok5Ay-"
   },
   "source": [
    "Let $x_i \\in \\mathrm{R}^d$ be the distributed representation of word $i$ in the source language, and $y_i \\in \\mathrm{R}^d$ is the vector representation of its translation. Our purpose is to learn such linear transform $W$ that minimizes euclidian distance between $Wx_i$ and $y_i$ for some subset of word embeddings. Thus we can formulate so-called Procrustes problem:\n",
    "\n",
    "$$W^*= \\arg\\min_W \\sum_{i=1}^n||Wx_i - y_i||_2$$\n",
    "or\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F$$\n",
    "\n",
    "where $||*||_F$ - Frobenius norm.\n",
    "\n",
    "In Greek mythology, Procrustes or \"the stretcher\" was a rogue smith and bandit from Attica who attacked people by stretching them or cutting off their legs, so as to force them to fit the size of an iron bed. We make same bad things with source embedding space. Our Procrustean bed is target embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIXVybSz5Ay-"
   },
   "source": [
    "![embedding_mapping.png](https://github.com/yandexdataschool/nlp_course/raw/master/resources/embedding_mapping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrHlVT1T5Ay_"
   },
   "source": [
    "![procrustes.png](https://github.com/yandexdataschool/nlp_course/raw/master/resources/procrustes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdUTL71x5Ay_"
   },
   "source": [
    "But wait...$W^*= \\arg\\min_W \\sum_{i=1}^n||Wx_i - y_i||_2$ looks like simple multiple linear regression (without intercept fit). So let's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "I8jAtfQZ5Ay_"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "mapping = LinearRegression(fit_intercept=False)\n",
    "mapping = mapping.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w07nPa25AzA"
   },
   "source": [
    "Let's take a look at neigbours of the vector of word _\"серпень\"_ (_\"август\"_ in Russian) after linear transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "TpLTrF0s5AzA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('апрель', 0.8541285991668701),\n",
       " ('июнь', 0.8411202430725098),\n",
       " ('март', 0.839699387550354),\n",
       " ('сентябрь', 0.8359869122505188),\n",
       " ('февраль', 0.8329297304153442),\n",
       " ('октябрь', 0.8311845660209656),\n",
       " ('ноябрь', 0.8278923034667969),\n",
       " ('июль', 0.8234528303146362),\n",
       " ('август', 0.8120499849319458),\n",
       " ('декабрь', 0.8039003610610962)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
    "ru_emb.most_similar(august)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klg3GVTU5AzA"
   },
   "source": [
    "We can see that neighbourhood of this embedding cosists of different months, but right variant is on the ninth place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XwHgezI5AzA"
   },
   "source": [
    "As quality measure we will use precision top-1, top-5 and top-10 (for each transformed Ukrainian embedding we count how many right target pairs are found in top N nearest neighbours in Russian embedding space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "E1rpBr3S5AzA"
   },
   "outputs": [],
   "source": [
    "def precision(pairs, mapped_vectors, topn=1):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
    "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
    "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
    "    :returns:\n",
    "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
    "    \"\"\"\n",
    "    assert len(pairs) == len(mapped_vectors)\n",
    "    num_matches = 0\n",
    "    for i, (_, ru) in enumerate(pairs):\n",
    "        similar = ru_emb.most_similar(mapped_vectors[i].reshape(1, -1))\n",
    "        for i in range(topn):\n",
    "            if similar[i][0] == ru:\n",
    "                num_matches += 1\n",
    "                break\n",
    "    precision_val = num_matches / len(pairs)\n",
    "    return precision_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "8EvlqaBC5AzB"
   },
   "outputs": [],
   "source": [
    "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "2fwaVkGS5AzB"
   },
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, X_test) == 0.0\n",
    "assert precision(uk_ru_test, Y_test) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6VuCOKGA5AzB"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'precision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-94b91cb251a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprecision_top1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muk_ru_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprecision_top5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muk_ru_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision_top5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mprecision_top1\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.635\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mprecision_top5\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.813\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'precision' is not defined"
     ]
    }
   ],
   "source": [
    "precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n",
    "precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)\n",
    "print(precision_top5)\n",
    "assert precision_top1 >= 0.635\n",
    "assert precision_top5 >= 0.813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AdPbM5t5AzB"
   },
   "source": [
    "## Making it better (orthogonal Procrustean problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXL4-XoH5AzC"
   },
   "source": [
    "It can be shown (see original paper) that a self-consistent linear mapping between semantic spaces should be orthogonal. \n",
    "We can restrict transform $W$ to be orthogonal. Then we will solve next problem:\n",
    "\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, where: } W^TW = I$$\n",
    "\n",
    "$$I \\text{- identity matrix}$$\n",
    "\n",
    "Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition. It turns out that optimal transformation $W^*$ can be expressed via SVD components:\n",
    "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
    "$$W^*=UV^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "id": "LQYFgFCh5AzE"
   },
   "outputs": [],
   "source": [
    "def learn_transform(X_train, Y_train):\n",
    "    \"\"\" \n",
    "    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n",
    "    \"\"\"   \n",
    "    svd = TruncatedSVD(n_components=299)\n",
    "    X_transformed = svd.fit_transform(X_train.transpose().dot(Y_train))\n",
    "    U = X_transformed / svd.singular_values_\n",
    "    VT = svd.components_\n",
    "    \n",
    "    return U.dot(VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "id": "7wd-JIB55AzE"
   },
   "outputs": [],
   "source": [
    "W = learn_transform(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "id": "y-bFJQSu5AzF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('апрель', 0.823787271976471),\n",
       " ('сентябрь', 0.8049144744873047),\n",
       " ('март', 0.8031120300292969),\n",
       " ('июнь', 0.8025237321853638),\n",
       " ('октябрь', 0.8003054261207581),\n",
       " ('ноябрь', 0.7938957214355469),\n",
       " ('февраль', 0.7916916608810425),\n",
       " ('июль', 0.7914064526557922),\n",
       " ('август', 0.7889314889907837),\n",
       " ('декабрь', 0.7690962553024292)]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_emb.most_similar([np.matmul(uk_emb[\"серпень\"], W)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "id": "1k176ALc5AzF"
   },
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, np.matmul(X_test, W)) >= 0.653\n",
    "assert precision(uk_ru_test, np.matmul(X_test, W), 5) >= 0.824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P_d61y55AzG"
   },
   "source": [
    "## UK-RU Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8W43owg5AzG"
   },
   "source": [
    "Now we are ready to make simple word-based translator: for earch word in source language in shared embedding space we find the nearest in target language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "id": "yc0D9IBE5AzG"
   },
   "outputs": [],
   "source": [
    "with open(\"fairy_tale.txt\", \"r\", encoding=\"utf-8\") as inpf:\n",
    "    uk_sentences = [line.rstrip().lower() for line in inpf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "id": "lHkSNCIg5AzG"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        sentence - sentence in Ukrainian (str)\n",
    "    :returns:\n",
    "        translation - sentence in Russian (str)\n",
    "\n",
    "    * find ukrainian embedding for each word in sentence\n",
    "    * transform ukrainian embedding vector\n",
    "    * find nearest russian word and replace\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    translation = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            most_similar = ru_emb.most_similar([np.matmul(uk_emb[word], W)])\n",
    "        except KeyError:\n",
    "            translation.append(word)\n",
    "        translation.append(most_similar[0][0])\n",
    "    s = \" \".join(translation)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "id": "MdnLiXS05AzH"
   },
   "outputs": [],
   "source": [
    "assert translate(\".\") == \".\"\n",
    "assert translate(\"1 , 3\") == \"1 , 3\"\n",
    "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "id": "Tz_gst5h5AzH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: лисичка - сестричка і вовк - панібрат\n",
      "dst: лисичка – сестричка и волк – панібрат –\n",
      "\n",
      "src: як була собі лисичка та зробила хатку, та й живе. а це приходять холоди. от лисичка замерзла та й побігла в село вогню добувать, щоб витопити. прибігає до одної баби та й каже:\n",
      "dst: как была себе лисичка и сделала хатку, сделала и и живе. и а оно приходят холоди. приходят из лисичка замерзла и и побежала во селение огня добувать, огня чтобы витопити. чтобы прибегает к одной бабы и и каже: и\n",
      "\n",
      "src: — здорові були, бабусю! з неділею... позичте мені огню, я вам одслужу.\n",
      "dst: — здоровые були, здоровые бабусю! здоровые со неділею... со позичте со мне огню, мне мной тебе одслужу. тебе\n",
      "\n",
      "src: — добре, — каже, — лисичко - сестричко. сідай погрійся трохи, поки я пиріжечки повибираю з печі!\n",
      "dst: — добре, — — каже, — — лисичко — – сестричко. – садись погрійся садись трохи, садись пока мной пирожки повибираю пирожки со печі! со\n",
      "\n",
      "src: а баба макові пиріжки пекла. от баба вибирає пиріжки та на столі кладе, щоб прохололи; а лисичка підгляділа та за пиріг, та з хати... виїла мачок із середини, а туди напхала сміттячка, стулила та й біжить.\n",
      "dst: а бабка маковые пирожки пекла. пирожки из бабка выбирает пирожки и по столе кладе, столе чтобы прохололи; чтобы а лисичка підгляділа лисичка и за пиріг, за и со хати... со виїла со мачок со со середини, со а туда напхала туда сміттячка, туда стулила туда и и біжить. и\n",
      "\n",
      "src: от біжить, а хлопці товар женуть до води.\n",
      "dst: из біжить, из а парни товар гонят к воды\n",
      "\n",
      "src: — здорові були, хлопці!\n",
      "dst: — здоровые були, здоровые хлопці! здоровые\n",
      "\n",
      "src: — здорова, лисичко - сестричко!\n",
      "dst: — здорова, — лисичко — – сестричко! –\n",
      "\n",
      "src: — проміняйте мені бичка - третячка на маковий пиріжок!\n",
      "dst: — проміняйте — мне бычка – третячка – по маковый пиріжок! маковый\n",
      "\n",
      "src: — добре, — кажуть.\n",
      "dst: — добре, — — кажуть. —\n",
      "\n",
      "src: — тільки, — каже, — тепер не їжте, а як я вибіжу з села, то тоді.\n",
      "dst: — тільки, — — каже, — — теперь не їжте, не а как мной вибіжу мной со села, со то тоді. то\n",
      "\n",
      "src: от помінялись. лисичка за бичка — та в ліс. а хлопці до пиріжка — а там сміттянко.\n",
      "dst: из помінялись. из лисичка за бычка — и во сельско а парни к пирожка — а там сміттянко. там\n",
      "\n",
      "src: от прибігла лисичка до своєї хатки, вирубала дерево, зробила санки, запрягла бичка — іде. аж біжить вовк:\n",
      "dst: из прибежала лисичка к своего хатки, своего вирубала своего дерево, своего сделала санки, сделала запрягла сделала бычка — іде. — аж бежит вовк: бежит\n",
      "\n",
      "src: — здорова, лисичко - сестричко!\n",
      "dst: — здорова, — лисичко — – сестричко! –\n",
      "\n",
      "src: — здоров, вовчику - братику!\n",
      "dst: — здоров, — вовчику — – братику! –\n",
      "\n",
      "src: — де ти взяла бичка - третячка і санки?\n",
      "dst: — куда ты взяла бычка – третячка – и санки? и\n",
      "\n",
      "src: — зробила собі!\n",
      "dst: — сделала собі! сделала\n",
      "\n",
      "src: — підвези ж, — каже, — мене, лисичко - сестричко!\n",
      "dst: — підвези — ж, — — каже, — — мене, — лисичко — – сестричко! –\n",
      "\n",
      "src: — е, куди я тебе візьму? ти мені й санки поламаєш!\n",
      "dst: — е, — куда мной тебя візьму? тебя ты мне и санки поламаєш! санки\n",
      "\n",
      "src: — ні, — каже, — я тільки одну ніжку положу.\n",
      "dst: — ні, — — каже, — — мной только одну ножку положу. ножку\n",
      "\n",
      "src: — ну, клади!\n",
      "dst: — ну, — клади! —\n",
      "\n",
      "src: от од’їхали трохи, вовк і каже:\n",
      "dst: из од’їхали из трохи, из волк и каже: и\n",
      "\n",
      "src: — положу я, лисичко - сестричко, й другу ніжку!\n",
      "dst: — положу — я, — лисичко — – сестричко, – и вторую ніжку! вторую\n",
      "\n",
      "src: — е, вовчику - братику, ти мені й санки поламаєш!\n",
      "dst: — е, — вовчику — – братику, – ты мне и санки поламаєш! санки\n",
      "\n",
      "src: — ні, — каже, — не поламаю!\n",
      "dst: — ні, — — каже, — — не поламаю! не\n",
      "\n",
      "src: — ну, клади!\n",
      "dst: — ну, — клади! —\n",
      "\n",
      "src: вовк і положив. ідуть, їдуть, коли це щось — трісь.\n",
      "dst: волк и положив. и ідуть, и їдуть, и когда оно что-то — трісь. —\n",
      "\n",
      "src: — е, вовчику - братику, ти мені вже й санки ламаєш!\n",
      "dst: — е, — вовчику — – братику, – ты мне уже и санки ламаєш! санки\n",
      "\n",
      "src: — ні лисичко - сестричко, то я орішок розкусив.\n",
      "dst: — ни лисичко ни – сестричко, – то мной орішок мной розкусив. мной\n",
      "\n",
      "src: — ну, гляди!\n",
      "dst: — ну, — гляди! —\n",
      "\n",
      "src: от їдуть...\n",
      "dst: из їдуть... из\n",
      "\n",
      "src: — положу я, лисичко - сестричко, й третю ніжку! — каже вовк.\n",
      "dst: — положу — я, — лисичко — – сестричко, – и третью ніжку! третью — говорит вовк. говорит\n",
      "\n",
      "src: — куди ти положиш? ти мені санки поламаєш! чим я тоді дровець привезу?\n",
      "dst: — куда ты положиш? ты ты мне санки поламаєш! санки Чем мной тогда дровець тогда привезу? тогда\n",
      "\n",
      "src: — ні, — каже, — не поламаю.\n",
      "dst: — ні, — — каже, — — не поламаю. не\n",
      "\n",
      "src: — ну, клади.\n",
      "dst: — ну, — клади. —\n",
      "\n",
      "src: вовк положив і третю ногу. коли це — трісь!\n",
      "dst: волк положил и третью ногу. третью когда оно — трісь! —\n",
      "\n",
      "src: — ой лихо! — каже лисичка. — йди собі геть, вовчику, — ти мені зовсім санки поламаєш.\n",
      "dst: — ой лихо! ой — говорит лисичка. говорит — иди себе геть, себе вовчику, себе — ты мне совсем санки поламаєш. санки\n",
      "\n",
      "src: — ні, то я орішок розкусив!\n",
      "dst: — ні, — то мной орішок мной розкусив! мной\n",
      "\n",
      "src: — дай же й мені!\n",
      "dst: — дай то и мені! и\n",
      "\n",
      "src: — нема, — каже, — останній!\n",
      "dst: — нема, — — каже, — — останній! —\n",
      "\n",
      "src: їдуть собі та й їдуть; вовчик і каже:\n",
      "dst: едут себе и и їдуть; и вовчик и и каже: и\n",
      "\n",
      "src: — сяду я зовсім, лисичко!\n",
      "dst: — сяду мной зовсім, мной лисичко! мной\n",
      "\n",
      "src: — куди ти сядеш, вовчику - братику? і санки розламаєш!\n",
      "dst: — куда ты сядеш, ты вовчику ты – братику? – и санки розламаєш! санки\n",
      "\n",
      "src: — я помаленьку, — каже.\n",
      "dst: — мной помаленьку, мной — каже. —\n",
      "\n",
      "src: — ну, гляди!\n",
      "dst: — ну, — гляди! —\n",
      "\n",
      "src: от вовчик тільки що сів, а санки так і розпались... лисичка тоді давай його лаять! лаяла - лаяла та й каже:\n",
      "dst: из вовчик из только что сів, что а санки так и розпались... и лисичка тогда давай его лаять! его ругала – ругала и и каже: и\n",
      "\n",
      "src: — піди ж, сякий - такий сину, дровець нарубай і на санки вирубай, і приволочи!\n",
      "dst: — пойди ж, пойди сякий пойди – такой сину, такой дровець такой нарубай такой и по санки вирубай, санки и приволочи! и\n",
      "\n",
      "src: — як же я, — каже вовчик, — вирубаю, коли я не знаю, якого дерева?\n",
      "dst: — как то я, то — говорит вовчик, говорит — вирубаю, — когда мной не знаю, не которого дерева? которого\n",
      "\n",
      "src: — е, — сякий - такий сину! як санчата ламать, так і знав, а дровець вирубать, то й ні!\n",
      "dst: — е, — — сякий — – такой сину! такой как санки ламать, санки так и знав, и а дровець а вирубать, а то и ні! и\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'most_similar' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-396-7af83f311978>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muk_sentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"src: {}\\ndst: {}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-394-7120be6fe893>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mtranslation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtranslation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'most_similar' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for sentence in uk_sentences:\n",
    "    print(\"src: {}\\ndst: {}\\n\".format(sentence, translate(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn7QjAAv5AzI"
   },
   "source": [
    "Not so bad, right? We can easily improve translation using language model and not one but several nearest neighbours in shared embedding space. But next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ug-wXcVt5AzI"
   },
   "source": [
    "## Would you like to learn more?\n",
    "\n",
    "### Articles:\n",
    "* [Exploiting Similarities among Languages for Machine Translation](https://arxiv.org/pdf/1309.4168)  - entry point for multilingual embedding studies by Tomas Mikolov (the author of W2V)\n",
    "* [Offline bilingual word vectors, orthogonal transformations and the inverted softmax](https://arxiv.org/pdf/1702.03859) - orthogonal transform for unsupervised MT\n",
    "* [Word Translation Without Parallel Data](https://arxiv.org/pdf/1710.04087)\n",
    "* [Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion](https://arxiv.org/pdf/1804.07745)\n",
    "* [Unsupervised Alignment of Embeddings with Wasserstein Procrustes](https://arxiv.org/pdf/1805.11222)\n",
    "\n",
    "### Repos (with ready-to-use multilingual embeddings):\n",
    "* https://github.com/facebookresearch/MUSE\n",
    "\n",
    "* https://github.com/Babylonpartners/fastText_multilingual -"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "homework.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
